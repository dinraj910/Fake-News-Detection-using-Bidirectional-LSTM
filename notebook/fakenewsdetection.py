# -*- coding: utf-8 -*-
"""FakeNewsDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/128YqmkzoqyHR6-AqNRr4f4Q1pgmuLAaV

Install Dependencies
"""

!pip install kaggle tensorflow scikit-learn matplotlib seaborn

"""Upload Kaggle API Key"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Download Dataset via CLI"""

!kaggle datasets download -d saurabhshahane/fake-news-classification
!unzip fake-news-classification.zip

"""Imports"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import (
    Embedding, LSTM, Bidirectional,
    Dense, Dropout, SpatialDropout1D
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

"""Load Dataset"""

df = pd.read_csv("WELFake_Dataset.csv")

print(df.head())
print(df.shape)

"""Basic Cleaning"""

df = df.dropna()
df = df.drop_duplicates()

print(df.shape)
print(df['label'].value_counts())

df.columns

"""Combine Title + Text"""

df['content'] = df['title'] + " " + df['text']

"""Length Distribution Analysis"""

df['length'] = df['content'].apply(lambda x: len(x.split()))

plt.figure(figsize=(10,5))
sns.histplot(df['length'], bins=100)
plt.xlim(0,1500)
plt.show()

print("Average length:", df['length'].mean())
print("95th percentile:", np.percentile(df['length'], 95))

MAX_LEN = 500

"""Tokenization"""

VOCAB_SIZE = 20000

tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(df['content'])

sequences = tokenizer.texts_to_sequences(df['content'])
padded = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')

X = padded
y = df['label'].values

print("X shape:", X.shape)

"""Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

"""Model Architecture"""

model = Sequential([
    Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),
    SpatialDropout1D(0.2),
    Bidirectional(LSTM(128, return_sequences=False)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(
    loss='binary_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

model.summary()

"""Callbacks"""

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=2,
    verbose=1
)

"""Train Model"""

history = model.fit(
    X_train,
    y_train,
    validation_split=0.1,
    epochs=10,
    batch_size=128,
    callbacks=[early_stop, lr_scheduler]
)

"""Evaluation"""

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

print(classification_report(y_test, y_pred))

print("ROC-AUC:", roc_auc_score(y_test, y_pred_prob))

"""Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(5,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Save Model"""

model.save("fake_news_bilstm.h5")

import pickle
with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

"""Testing Model"""

y_pred_prob = model.predict(X_test)
print("Mean prediction:", np.mean(y_pred_prob))
print("Min:", np.min(y_pred_prob))
print("Max:", np.max(y_pred_prob))

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, (y_pred_prob > 0.5))
print(cm)

# Get misclassified examples
misclassified = X_test[(y_pred.flatten() != y_test)]

print("Total misclassified:", len(misclassified))

import tensorflow as tf
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the saved model
loaded_model = tf.keras.models.load_model("fake_news_bilstm.h5")

# Load the tokenizer
with open("tokenizer.pkl", "rb") as f:
    loaded_tokenizer = pickle.load(f)

# Define MAX_LEN (should be the same as used during training)
# MAX_LEN = 500  # This was already defined in the notebook

def predict_news(text):

    sequence = loaded_tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(
        sequence,
        maxlen=MAX_LEN,
        padding='post',
        truncating='post'
    )

    prediction = loaded_model.predict(padded_sequence)[0][0]

    if prediction > 0.5:
        return "Real News", prediction
    else:
        return "Fake News", prediction


# Example usage:
example_news_fake = "Trump says Brexit to be 'a great thing', wants quick trade deal with UK. LONDON (Reuters) - U.S. President-elect Donald Trump said that Brexit would turn out to be a great thing wants quick trade edal with UK "
example_news_real = "THE ROBBING OF INNOCENCE: 12 Yr Old Students Given CDC Survey About Transgender, Gay And Oral Sex.The left believes these are all perfectly acceptable topics to discuss with our young children. "
example_news_fake_1 = "U.S. House votes to slap back Trump’s tariffs on Canada. The House has voted to slap back President Donald Trump’s tariffs on Canada, a rare if largely symbolic rebuke of the White House agenda as Republicans joined Democrats over the objections of GOP leadership."

result_fake, prob_fake = predict_news(example_news_fake)
result_real, prob_real = predict_news(example_news_real)
result_fake_1, prob_fake_1 = predict_news(example_news_fake_1)

print(f"'{{example_news_fake}}' is predicted as: {result_fake} (Probability: {prob_fake:.4f})")
print(f"'{{example_news_real}}' is predicted as: {result_real} (Probability: {prob_real:.4f})")
print(f"'{{example_news_fake_1}}' is predicted as: {result_fake_1} (Probability: {prob_fake_1:.4f})")

plt.hist(y_pred_prob, bins=50)
plt.title("Prediction Probability Distribution")
plt.show()

"""### Analysis of Misclassification

Upon inspecting the tokenized words for the real news article "Latest Pipeline Leak Underscores Dangers Of Dakota Access Pipeline", we observe the following:

1.  **Vocabulary Overlap**: Words like 'leak', 'underscores', and 'dangers' convey a negative or alarming tone. While legitimate for real news concerning incidents, such words are also frequently used in sensationalized or fake news to grab attention. The model might have picked up on these terms as indicators of fake news due to their prevalence in the 'fake' class during training.
2.  **Short Length**: The input text is very short (only 9 tokens). Shorter texts often provide less context, making it harder for the model to accurately classify them. With limited information, the model relies more heavily on individual word associations.
3.  **Lack of Contextual Clues**: The article title, while real, doesn't contain typical markers of official, factual reporting that might distinguish it from fake news (e.g., specific dates, names of official bodies, objective reporting language). The model might be trained on more extensive articles where such contextual cues are present.
4.  **Model Bias**: It's possible that the training data for fake news contained many examples with similar 'alarming' vocabulary, leading the model to develop a bias towards classifying short, negative-sounding headlines as fake.

**Summary:** The model likely misclassified the real news article as fake due to a combination of its short length, the presence of negatively charged words commonly found in sensationalized content, and a potential bias learned from the training data where similar vocabulary might have been strongly associated with fake news.

### Suggested Next Steps for Improvement

To address the observed misclassification and improve the model's performance, especially for short, real news headlines with potentially negative connotations, consider the following:

1.  **Refine Training Data**: Analyze the distribution of vocabulary in both real and fake news within the training dataset. If negative or alarming words are disproportionately present in fake news samples, consider:
    *   **Augmenting Real News**: Introduce more real news articles that discuss negative events or incidents to balance the vocabulary representation.
    *   **Feature Engineering**: Create features that capture the sentiment or tone of a text, and explicitly guide the model to differentiate between legitimate negative reporting and sensationalist fake news.
2.  **Increase Context**: For very short texts, the model lacks sufficient context. Possible solutions include:
    *   **Including Article Body**: If available, incorporate the full article body during training and prediction, not just the title. This would provide more comprehensive information for classification.
    *   **External Knowledge**: Integrate external knowledge (e.g., entity recognition, source reliability) as additional features to provide more context about the news item.
3.  **Advanced Embeddings**: Experiment with more sophisticated word embeddings (e.g., Word2Vec, GloVe, FastText) or contextual embeddings (e.g., BERT, GPT-2) that capture semantic nuances better than simple Tokenizer embeddings. These models are often better at understanding the context in which words are used.
4.  **Ensemble Methods**: Combine multiple models (e.g., one focusing on sentiment, another on linguistic style, and the current Bidirectional LSTM) to make a more robust prediction. Each model could contribute its strength to reduce misclassifications.
5.  **Hyperparameter Tuning & Regularization**: Further fine-tune the model's hyperparameters and experiment with different regularization techniques (e.g., dropout rates, L1/L2 regularization) to prevent overfitting to spurious correlations in the training data.
6.  **Error Analysis**: Perform a deeper error analysis on a larger set of misclassified samples to identify common patterns beyond vocabulary, such as specific linguistic styles, sentence structures, or themes that confuse the model.

## Final Task

### Subtask:
Provide a summary of the analysis, explaining the most likely reasons for the misclassification of 'example_news_real' and suggesting possible next steps for improvement.

## Summary:

### Q&A
The real news article "Latest Pipeline Leak Underscores Dangers Of Dakota Access Pipeline" was misclassified as "Fake News" with a high probability (0.9998) primarily due to:
*   **Vocabulary Overlap:** The presence of negatively charged words such as 'leak', 'underscores', and 'dangers', which are common in both legitimate reporting of incidents and sensationalized fake news.
*   **Short Length:** The article's title, being very short (9 tokens), provided insufficient context for accurate classification, leading the model to rely heavily on individual word associations.
*   **Lack of Contextual Clues:** The title lacked typical markers of factual reporting (e.g., dates, official bodies) that could distinguish it from fake news, especially if the model was trained on longer articles with such cues.
*   **Model Bias:** A potential bias learned during training where similar alarming vocabulary might have been strongly associated with fake news.

### Data Analysis Key Findings
*   The model confirmed its initial misclassification of the real news article "Latest Pipeline Leak Underscores Dangers Of Dakota Access Pipeline" as "Fake News" with a high probability of 0.9998.
*   The tokenized text revealed the specific words the model processed: 'latest', 'pipeline', 'leak', 'underscores', 'dangers', 'of', 'dakota', 'access', 'pipeline'.
*   The article title consists of only 9 significant tokens, which is a very short text for classification.

### Insights or Next Steps
*   **Enhance Training Data and Context:** To mitigate misclassifications due to vocabulary overlap and short text length, augment the training data with more real news examples that contain negative or alarming terms, and consider incorporating the full article body or external contextual information during both training and prediction.
*   **Explore Advanced Model Techniques:** Improve model robustness by experimenting with more sophisticated word and contextual embeddings (e.g., Word2Vec, BERT) to capture semantic nuances more effectively, or by utilizing ensemble methods to combine predictions from multiple models.

test scraping
"""

!pip install newspaper3k lxml_html_clean

"""URL Scraping Function"""

from newspaper import Article

def scrape_article_first_para(url):
    try:
        article = Article(url)
        article.download()
        article.parse()

        title = article.title
        full_text = article.text

        if not full_text:
            return None, None, "Could not extract article text."

        # Split into paragraphs
        paragraphs = full_text.split("\n")

        # Remove very short paragraphs (noise)
        paragraphs = [p.strip() for p in paragraphs if len(p.split()) > 20]

        if len(paragraphs) == 0:
            return None, None, "No valid paragraph found."

        first_paragraph = paragraphs[0]

        return title, first_paragraph, None

    except Exception as e:
        return None, None, str(e)

"""Integrate With Your Model"""

def predict_from_url(url):

    title, first_para, error = scrape_article_first_para(url)

    if error:
        return {"error": error}

    combined_text = title + " " + first_para

    sequence = loaded_tokenizer.texts_to_sequences([combined_text])
    padded = pad_sequences(
        sequence,
        maxlen=MAX_LEN,
        padding='post',
        truncating='post'
    )

    prediction = loaded_model.predict(padded)[0][0]

    if prediction > 0.5:
        label = "Real News"
    else:
        label = "Fake News"

    return {
        "title": title,
        "first_paragraph": first_para,
        "prediction": label,
        "probability_real": float(prediction),
        "probability_fake": float(1 - prediction)
    }

"""Test Inside Notebook"""

url = "https://www.livemint.com/news/world/china-factory-ports-activity-lunar-new-year-trade-tariff-donald-trump-xi-jinping-meeting-11770877554544.html"

result = predict_from_url(url)

for key, value in result.items():
    print(f"{key}: {value}")

url = "https://www.thehindu.com/news/international/us-house-votes-to-slap-back-trumps-tariffs-on-canada/article70622321.ece"

result = predict_from_url(url)

for key, value in result.items():
    print(f"{key}: {value}")